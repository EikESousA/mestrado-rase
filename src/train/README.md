# Treinamento LoRA para Chatbot RAG em PortuguÃªs

## ğŸ“‹ DescriÃ§Ã£o

Este projeto treina modelos LoRA para fine-tuning leve usando diferentes modelos de linguagem:

- **Mistral 7B Instruct** quantizado em 4-bit (ideal para GPUs RTX 4070, 4080, 3060 e similares)
- **TinyLlama 3B** tambÃ©m com quantizaÃ§Ã£o 4-bit para GPUs menores
- **GPT-Neo 125M**, modelo leve para CPU (sem quantizaÃ§Ã£o), indicado para quem nÃ£o tem GPU dedicada e quer respostas rÃ¡pidas (2-3s)

O objetivo Ã© criar um chatbot RAG (Retrieval-Augmented Generation) em portuguÃªs usando datasets PirÃ¡ e SQuAD-BR 2.0, com contexto manual e sem ElasticSearch.

---

## ğŸ› ï¸ Requisitos

- Para Mistral 7B e TinyLlama 3B:
  - GPU Nvidia com pelo menos 12 GB de VRAM (RTX 3060, 4070, 4080 ou superior)
  - CUDA e drivers Nvidia atualizados
- Para GPT-Neo 125M:
  - CPU moderna (idealmente com mÃºltiplos nÃºcleos, ex. i5/i7 de Ãºltima geraÃ§Ã£o)
  - Roda sem GPU, porÃ©m com menor capacidade e velocidade menor para modelos maiores
- Python 3.10 ou 3.11
- EspaÃ§o em disco para datasets e checkpoints (~10 GB)
- ConexÃ£o com internet para baixar modelos e datasets

---

## ğŸ“¦ InstalaÃ§Ã£o

1. Clone o repositÃ³rio ou copie os arquivos para sua mÃ¡quina.

2. Crie e ative ambiente virtual Python (opcional, mas recomendado):

```bash
python -m venv venv
source venv/bin/activate    # Linux/macOS
venv\Scripts\activate       # Windows PowerShell
```

3. Instale as dependÃªncias:

```bash
pip install -r requirements.txt
```

---

## ğŸ—‚ï¸ Preparar datasets

Baixe e converta os datasets PirÃ¡ e SQuAD-BR para JSONL com:

```bash
python download_and_convert.py
```

Os arquivos `pira.jsonl` e `squad_br.jsonl` serÃ£o criados na pasta `data/`.

---

## ğŸ‹ï¸â€â™‚ï¸ Treinar o LoRA

O script `train_lora.py` foi adaptado para suportar diferentes modelos e configuraÃ§Ãµes automÃ¡ticas:

- **Modelos suportados**:
  - `mistral` (Mistral 7B 4-bit quantizado)
  - `tinyllama` (TinyLlama 3B 4-bit quantizado)
  - `gpt` (GPT-Neo 125M sem quantizaÃ§Ã£o, para CPU)

Execute o treino com:

```bash
python train_lora.py [modelo]
```

Exemplos:

```bash
python train_lora.py mistral
python train_lora.py tinyllama
python train_lora.py gpt
```

### ConfiguraÃ§Ãµes automÃ¡ticas

- Para Mistral e TinyLlama, o treino usa quantizaÃ§Ã£o 4-bit e `device_map="auto"` para usar GPU.
- Para GPT-Neo 125M, o modelo roda na CPU, sem quantizaÃ§Ã£o, com batch size e passos ajustados para menor uso de memÃ³ria.
- As configuraÃ§Ãµes de batch size, steps, fp16 etc sÃ£o ajustadas automaticamente com base no modelo e na GPU detectada.

---

## ğŸ¤– InferÃªncia com LoRA treinado

O script `test_infer.py` (ou seu script customizado) deve ser adaptado para carregar o modelo base e o LoRA conforme o modelo escolhido:

- Para Mistral e TinyLlama, use quantizaÃ§Ã£o 4-bit com `BitsAndBytesConfig` e `device_map="auto"`.
- Para GPT-Neo 125M, carregue o modelo sem quantizaÃ§Ã£o e em CPU (`device_map="cpu"`).
- Envie os tensores para CUDA somente se GPU disponÃ­vel e modelo grande.

Exemplo de comando para rodar inferÃªncia:

```bash
python test_infer.py [modelo]
```

---

## ğŸ“Š Monitorar uso de VRAM (opcional)

- Windows PowerShell:

```powershell
.	rain_monitor.bat
```

- Linux/bash:

```bash
./train_monitor.sh
```

Esses scripts ajudam a monitorar o uso da GPU e sugerem ajustes no batch size para evitar crashes por falta de memÃ³ria.

---

## âš™ï¸ Notas importantes

- **QuantizaÃ§Ã£o 4-bit (bitsandbytes)** Ã© usada somente para modelos grandes (Mistral 7B, TinyLlama 3B) para reduzir consumo de VRAM.
- **GPT-Neo 125M** nÃ£o usa quantizaÃ§Ã£o, roda na CPU, Ã© ideal para quem nÃ£o tem GPU dedicada.
- Ajuste os parÃ¢metros do treino conforme seu hardware para otimizar desempenho e evitar erros.
- Para produÃ§Ã£o, considere usar RAG com ElasticSearch ou outro indexador para melhorar contexto e respostas.

---

## ğŸ“‚ Estrutura dos arquivos

```
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ pira.jsonl
â”‚   â””â”€â”€ squad_br.jsonl
â”œâ”€â”€ lora_out/               # modelo LoRA treinado serÃ¡ salvo aqui
â”œâ”€â”€ train_lora.py           # script de treino LoRA multi-modelo e multi-device
â”œâ”€â”€ test_infer.py           # script para testar inferÃªncia adaptado para mÃºltiplos modelos
â”œâ”€â”€ download_and_convert.py # script para baixar e converter datasets
â”œâ”€â”€ train_monitor.ps1       # monitor VRAM PowerShell Windows
â”œâ”€â”€ train_monitor.sh        # monitor VRAM Bash Linux
â”œâ”€â”€ train_monitor.bat       # monitor VRAM bÃ¡sico Windows cmd
â”œâ”€â”€ requirements.txt        # dependÃªncias Python
â””â”€â”€ README.md               # este arquivo
```

---

## ğŸ’¡ Dicas

- Sempre monitore a VRAM para ajustar batch size e evitar crashes.
- Use 4-bit para balancear qualidade e desempenho em GPUs.
- Em CPU, prefira modelos leves como GPT-Neo 125M para respostas rÃ¡pidas (2-3s).
- Adapte o prompt e o dataset conforme seu caso para melhorar resultados.

---

Se precisar, posso ajudar a criar um guia de inferÃªncia com esses modelos tambÃ©m!

http://www.wikicfp.com/cfp/call?conference=computer%20science&page=2
https://ppgcc.github.io/discentesPPGCC/pt-BR/qualis/
